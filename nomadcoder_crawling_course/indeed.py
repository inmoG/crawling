import requests
from bs4 import BeautifulSoup

LIMIT = 50
URL = f"https://kr.indeed.com/jobs?q=Python&l=&limit={LIMIT}"


def get_last_pages():
    result = requests.get(URL)
    soup = BeautifulSoup(result.text, "html.parser")
    pagination = soup.find("div", {"class": "pagination"})  # this is list
    links = pagination.find_all("a")

    page = []

    for link in links[:-1]:  # 다음"은 스트링이여서 정수로 변환이 불가함 아예 제외하고 for문을 돌리는 것
        page.append(int(link.string))

    MAX_page = page[-1]  # last_page
    return MAX_page
    # 페이지가 0~12번까지라면 12번은 안나옴
    # anchor(요소)안에 요소가 하나뿐이며 스트링만 가지고 있으면 anchor(요소)만  뽑아와도 스트링을 뽑아줌
    # [2,3,4,5,6,7,8,9,10,11,next]
    # reverse [next,11,10,9,8,7,6,5,4,3,2] -1,-2,-3,-4,-5
    # -1 = next, -2 = 11, -3 = 10 을 뜻한다. 따라서 [:-1]은
    # -1인 next를 제외하고 뽑아달라는 의미다. -2 까지라면 11, next를 제외한다.


def extract_job(html):
    title = html.find("div", {"class": "title"}).find("a")["title"]
    company = html.find("span", {"class": "company"})
    company_anchor = company.find("a")
    location = html.find("div", {"class": "recJobLoc"})["data-rc-loc"]
    job_id = html["data-jk"]

    if company:
        if company_anchor is not None:
            company = str(company_anchor.string)
        else:
            company = str(company.string)
        ompany = company.strip()
    else:
        company = None

    return {
        "title": title,
        "company": company,
        "location": location,
        "link": f"https://kr.indeed.com/viewjob?jk={job_id}",
    }


def extract_jobs(last_page):
    jobs = []
    for page in range(last_page):
        print(f"indeed Web Scrraping Page : {page}")
        result = requests.get(f"{URL}&start={page*LIMIT}")
        # 페이지당 50개의 구인공고를 출력하고 페이지가  넘어갈 때 마다 start = page * 50
        soup = BeautifulSoup(result.text, "html.parser")
        results = soup.find_all("div", {"class": "jobsearch-SerpJobCard"})
        # results는 Soup의 리스트다.
        # find all = 모든 리스트를 가져온다
        # find = 첫번째 찾은 결과를 보여준다.
        for result in results:
            job = extract_job(result)
            jobs.append(job)
    return jobs


def get_jobs():
    last_page = get_last_pages()
    jobs = extract_jobs(last_page)
    return jobs
